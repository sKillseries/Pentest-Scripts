#!/bin/bash

url=$1
fqdn=$(echo $url | sed 's#^https://www\.\(.*\)/$#\1#')

if [ ! -d "resultats" ];then
    mkdir resultats
fi
if [ ! -d "files_to_process" ];then
    mkdir files_to_process
fi
if [ ! -d "resultats/webrecon.txt" ];then
    touch resultats/webrecon.txt
fi
if [ ! -f "resultats/webalive.txt" ];then
    touch resultats/webalive.txt
fi
if [ ! -f "resultats/webfinal.txt" ];then
    touch resultats/webfinal.txt
fi
if [ ! -f "resultats/webassets" ];then
    touch resultats/webassets.txt
fi
if [ ! -f "resultats/web_potential_takeovers.txt" ];then
    touch resultats/web_potential_takeovers.txt
fi

echo "[+] HTTP response headers checking..."
{
    echo "[#] Curl Result"
    curl --location --head "$url"
    echo -e "\n"
} >> resultats/webrecon.txt

curl --location --head "$url" -o files_to_process/web_curl.xml

echo "[+] Site crawling..."
{
    echo "[#] hakrawler Result"
    echo "$url" | hakrawler -d 10
    echo -e "\n"
} >> resultats/webrecon.txt

echo "$url" | hakrawler -d 10 | jq -R -s -c '{urls: split("\n")}' > files_to_process/web_hakrawler.json

echo "[+] Harvesting subdomains with assetfinder..."
assetfinder "$fqdn" >> resultats/webassets.txt
sort -u resultats/webassets.txt >> resultats/webfinal.txt
rm resultats/webassets.txt

echo "[+] Double checking for subdomains with amass..."
amass enum -d "$fqdn" >> resultats/webf.txt
sort -u resultats/webf.txt >> resultats/webfinal.txt
rm resultats/webf.txt

echo "[+] dnsrecon enumeration and zone transfer..."
{
    echo "[#] dnsrecon result"
    dnsrecon -a -d "$url"
    echo -e "\n"
} >> resultats/webrecon.txt

dnsrecon -a -d "$url" | jq -R -s -c 'split("\n") | map(split(",") | {key: .[0], value: .[1]})' > files_to_process/web_dnsrecon.json

echo "[+] Probing for alive domains..."
sort -u resultats/webfinal.txt | httprobe -s -p https:443 | sed 's/https\?:\/\///' | tr -d ':443' >> resultats/weba.txt
sort -u resultats/weba.txt >> resultats/webalive.txt
sort -u resultats/weba.txt | jq -R -s -c 'split("\n") | {urls: .}' > files_to_process/webalive.json
rm resultats/weba.txt
rm resultats/webfinal.txt
 
echo "[+] Checking for possible subdomain takeover..." 
subjack -w resultats/webalive.txt -t 100 -timeout 30 -ssl -c ~/go/src/github.com/haccer/subjack/fingerprints.json -v 3 -o resultats/web_potential_takeovers.txt
subjack -w resultats/webalive.txt -t 100 -timeout 30 -ssl -c ~/go/src/github.com/haccer/subjack/fingerprints.json -v 3 -o - | jq -R -s -c 'split("\n") | {subdomains: .}' > files_to_process/web_subjack.json
{
    echo "[#] potential subdomain takeover"
    cat resultats/web_potential_takeovers.txt
    echo -e "\n"
} >> resultats/webrecon.txt
rm resultats/webalive.txt
rm resultats/web_potential_takeovers.txt
 
echo "[+] Scanning for open ports..."
{
    echo "[#] nmap web alive result"
    nmap -iL resultats/webalive.txt
    echo -e "\n"
} >> resultats/webrecon.txt

nmap -iL resultats/webalive.txt -oX files_to_process/web_nmap_scan.xml

echo "[+] WAF checking..."
{
    echo "[#] Wafw00f result"
    wafw00f "$url"
    echo -e "\n"
} >> resultats/webrecon.txt

wafw00f "$url" | jq '.' > files_to_process/web_wafw00f.json

echo "[+] Double WAF checking..."
{
    echo "[#] Whatwaf result"
    whatwaf -u "$url"
    echo -e "\n"
} >> resultats/webrecon.txt

whatwaf -u "$url" 2>&1 | jq -R -s -c 'split("\n") | {results: .}' > files_to_process/web_whatwaf.json

echo "[+] CMS identification checking..."
{
    echo "[#] droopescan result"
    droopescan scan -u "$url"
    echo -e "\n"
} >> resultats/webrecon.txt

droopescan scan -u "$url" | jq '.' > files_to_process/web_droopescan.json

echo "[+] vulnerability scanning"
{
    echo "nikto vulnerability scanning result"
    nikto -h "$url"
    echo -e "\n"
} >> resultats/webrecon.txt

nikto -h "$url" -o - | jq '.' > resultats/nikto_output.json

echo "[+] double checking vulnerability scanning"
{
    echo "wapiti vulnerability scanning result"
    wapiti -u "$url"
    echo -e "\n"
} >> resultats/webrecon.txt

wapiti -u "$url" -f xml -o files_to_process/wapiti.xml